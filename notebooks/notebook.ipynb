{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Notes for the Codes\n(Updated 27/06/2021)\n\n\nThere are 3 folders in the Google Drive: Old, New, Updown Clean. The Old folder contains the oldest model that I tried using buy-sell-hold output model, but it didn't perform very well, so it has been discontinued and I decided to make a new folder when trying the newer models. All the necessary codes (and even the unnecessary ones) have been moved to the New folder. I will try to give a clear description on what every codes in the New folder does. The codes in Updown Clean folder is the tidy ones, containing only the necessary codes for the currently best performing algorithm. Every number, algorithm, variables can be changed and feel free to edit, add, and/or improve the codes in whichever way you like and try different things.",
   "metadata": {
    "tags": [],
    "cell_id": "00000-fc5feb24-baed-4f09-8fbb-710b8a199638",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Pre-Training\n\nAll the python codes in the New folder with the execption of the ones containing the Neural Network (NN) only need the package numpy. (Some probably uses matplotlib for plotting but you may just delete the lines containing the plotting parts.) So they all can be ran easily. To run the ones containing the NN we need tensorflow package. I myself didn't install tensorflow on my laptop, instead I always run the NN codes in deepnote, where all the needed packages have already been installed. Deepnote is completely free and it provides practically unlimited number of projects and computing time for personal use. But, the number of the projects that can be added to the group is limited, but I think we can always share our personal projects on deepnote by allowing public access or publishing. The other codes in the folder are designed to do some simple tasks related to generating the input and the output training for the NN.\n\n",
   "metadata": {
    "tags": [],
    "cell_id": "00001-6bd574bd-b696-4c02-9785-fb836629f5a2",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 1.1. Generating the Input for the Training\nThe input for the neural network at the moment is the last `timesteps`=20 days of opening prices (should we change to adjusted closing instead?), volumes, and financial statement ratios. To train the NN, I took all the historical data for FB starting from 2013 from Bloomberg. I have also downloaded the historical data for AMZN stock, although the NN hasn't been trained on this data. The raw data obtained from Bloomberg is in the folder raw (`New/FB/raw`). Initially for FB I split the data for training (2013-2019), and examination (2019-2021). But there's already a python code that can split this, so in the future we can just take the data completely until today (see AMZN example). One problem is that financial data are released every 3 months while prices and volume are updated everyday, so to join these two data is the task of the program `financial_statements_trynew.py`. So the first thing to do after obtaining the data from Bloomberg is to make it into columns shape and changing the dates to integers with a fixed reference date (e.g. if the financial statement date is 27/06/2021 and ref. is 20/06/2021 than the integer is 7). These I did using excel, and the result is the two excel files in the folder `New/FB`. Next the excel data needs to exported to `.txt` (these are the files `price.txt` and `fs.txt` in the FB folder). For price the first column needs to be the date, for the financial statements the first column is the financial statements release date (I found on the internet and put it manually), and the second column is the financial statements date. Just count the number of the columns in each price and financial statements file and change line 10-17 (shown below) in the `financial_statements_trynew.py`. Run the python code and the output is already the price everyday and the latest financial statement data. The output file name is `final_input.txt`",
   "metadata": {
    "tags": [],
    "cell_id": "00002-71fad619-0b33-49cf-9fb5-f30c713c2743",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-4a33927e-1002-48f8-958f-7d631a89271d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d72bc678",
    "execution_start": 1624789937604,
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\n#FIRST COLUMN SHOULD BE THE NUMBER OF DAY, SCND THE OPENING PRICE, SIXTH THE VOLUME AND SO ON\nprice_date_txt = \"FB/price.txt\" \n#THE COUNT STARTS FROM VOLUME\nNcolumns_price = 7 \n#FIRST COLUMN IS FINANCIAL RELEASE DATE, THE SECOND SHOULD BE FINANCIAL STATEMENT DATE\nfs_txt = \"FB/fs.txt\" \n#UNLIKE ABOVE THIS INCLUDES THE DATES, SO MINIMUM IS 2\nNcolumns = 15 \nsave_txt = \"FB/final_input.txt\"",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1.2. Generating the Correct Answer for the Training\nThere are many codes for doing this depending on the model that we want to use. For the moment I'll probably just give notes on the most essential one. Basically in the most promising model (updown), the answer generator is the file `answer_updown.py`. All the others answer generator files are old ones using more complicated models that I tried during the earlier development. But all these sadly gave rather dissapointing results. This is a very short python codes, and the most important lines that you need to change are:\n",
   "metadata": {
    "tags": [],
    "cell_id": "00003-83373163-80a7-44a4-a3b4-e648fd922a6c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-c61967af-a5a6-4e3a-a5f9-f7c5067add5c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e9f222e1",
    "execution_start": 1624790583284,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "input_txt = \"FB/price.txt\"\noutput_txt = \"FB/correct_ans_updown3.txt\"\ndaysforward = 1\nprice_gap = 0",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "`daysforward` in the present model is 5, to determine if the price in the next week is increasing or decreasing and `price_gap` is 2 (2 dollar). It uses the second column in `FB/price.txt` which is the opening price. We probably need in the future to change the `price_gap` depending on the volatility of the share price (as a percent of the share price for example). The idea to introduce the price gap is simply to avoid teaching the NN to buy when the price is just increasing due to fluctuations.",
   "metadata": {
    "tags": [],
    "cell_id": "00006-305e20f9-6549-4a94-af17-9c48281169c7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 1.3. Generating the exam\nAnother important python file is `exam_generator.py`. It basically just take the last 500+`timesteps` data from the `final_input.txt` and `correct_ans_updown.txt`. We need to specify the number of columns in the `final_input.txt`.",
   "metadata": {
    "tags": [],
    "cell_id": "00007-faa97682-105e-4412-8bee-fd56aacc6040",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-55eb8ffb-a3be-4627-a28c-f2cc19e3145c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8bea25a9",
    "execution_start": 1624791170804,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "input_txt = \"FB/final_input.txt\"\nanswer_txt = \"FB/correct_ans_updown2.txt\"\noutput_txt = \"FB/exam_input.txt\"\noutput_txt2 = \"FB/exam_answer.txt\"\nn_input_data = 21\nexam_number = 500\ntimesteps=20",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### That's it\n\nThat's all the most essential things at the moment. I will probably make a more detailed notes on the detailed codes later on. But, basically these are enough to train the current model. All these files can be found in the folder Updown Clean, where only the necessary codes are there.",
   "metadata": {
    "tags": [],
    "cell_id": "00009-1ffd596f-cba3-453c-971f-94846484b5a9",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Training",
   "metadata": {
    "tags": [],
    "cell_id": "00010-f19414cd-d73e-491b-b4e3-0b856dbd3180",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Examination",
   "metadata": {
    "tags": [],
    "cell_id": "00011-a2aa50fb-385b-4a9f-bfd5-2d84d8c3f702",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=57d5357a-0ed2-4e20-a593-37074262833e' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "a577bac9-244e-4931-be50-78457cbc8393",
  "deepnote_execution_queue": []
 }
}